{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11676470,"sourceType":"datasetVersion","datasetId":7328424}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch-geometric transformers\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom transformers import BertTokenizer, BertModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:29:48.005020Z","iopub.execute_input":"2025-05-06T17:29:48.005830Z","iopub.status.idle":"2025-05-06T17:29:51.151774Z","shell.execute_reply.started":"2025-05-06T17:29:48.005798Z","shell.execute_reply":"2025-05-06T17:29:51.150978Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Replace with actual filename\ndf = pd.read_csv(\"/kaggle/input/personality-vad/Dyadic_PELD.tsv\", sep=\"\\t\")\n\n# Emotion map\nemotion2id = {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3,\n              'neutral': 4, 'sadness': 5, 'surprise': 6}\ndf[\"emotion_id\"] = df[\"Emotion_3\"].str.lower().map(emotion2id)\n\n# Stratified train/test split\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"emotion_id\"], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:30:21.699060Z","iopub.execute_input":"2025-05-06T17:30:21.699564Z","iopub.status.idle":"2025-05-06T17:30:21.750177Z","shell.execute_reply.started":"2025-05-06T17:30:21.699543Z","shell.execute_reply":"2025-05-06T17:30:21.749640Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load BERT\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\nbert_model.eval()\n\ndef get_bert_embedding(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    with torch.no_grad():\n        return bert_model(**inputs).last_hidden_state.mean(dim=1).squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:30:32.768833Z","iopub.execute_input":"2025-05-06T17:30:32.769108Z","iopub.status.idle":"2025-05-06T17:30:33.613387Z","shell.execute_reply.started":"2025-05-06T17:30:32.769078Z","shell.execute_reply":"2025-05-06T17:30:33.612635Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def personality_to_vad(P):\n    O, C, E, A, N = P\n    return np.array([\n        0.21 * E + 0.59 * A + 0.19 * N,\n        0.15 * O + 0.30 * A - 0.57 * N,\n        0.25 * O + 0.17 * C + 0.60 * E - 0.32 * A\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:30:42.558228Z","iopub.execute_input":"2025-05-06T17:30:42.558727Z","iopub.status.idle":"2025-05-06T17:30:42.562744Z","shell.execute_reply.started":"2025-05-06T17:30:42.558704Z","shell.execute_reply":"2025-05-06T17:30:42.561950Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def build_graph_version_a(row, verbose=False):\n    try:\n        P = eval(row['Personality']) if isinstance(row['Personality'], str) else row['Personality']\n        if not isinstance(P, (list, tuple)) or len(P) != 5:\n            if verbose: print(\" Invalid Personality:\", row['Personality'])\n            return None\n\n        U1 = get_bert_embedding(row['Utterance_1'])\n        U2 = get_bert_embedding(row['Utterance_2'])\n        M3 = get_bert_embedding(row['Utterance_3'])\n\n        if U1.shape[0] != 768 or U2.shape[0] != 768 or M3.shape[0] != 768:\n            if verbose: print(\" BERT shape issue\")\n            return None\n\n        P_node = F.pad(torch.tensor(personality_to_vad(P), dtype=torch.float), (0, 765), value=0)\n        x = torch.stack([U1, U2, P_node], dim=0)\n        edge_index = torch.tensor([[0, 2, 2], [1, 0, 1]], dtype=torch.long)\n        emotion = torch.tensor(emotion2id[row[\"Emotion_3\"].lower()], dtype=torch.long)\n\n        return Data(x=x, edge_index=edge_index, y=emotion, mood=M3)\n    except Exception as e:\n        if verbose: print(f\" Exception: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:30:57.149280Z","iopub.execute_input":"2025-05-06T17:30:57.149819Z","iopub.status.idle":"2025-05-06T17:30:57.156494Z","shell.execute_reply.started":"2025-05-06T17:30:57.149794Z","shell.execute_reply":"2025-05-06T17:30:57.155792Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Train graphs\ngraph_list_a = []\nfor i, (_, row) in enumerate(tqdm(train_df.iterrows(), total=len(train_df))):\n    g = build_graph_version_a(row, verbose=(i < 3))  # debug first 3\n    if g: graph_list_a.append(g)\n\n# Test graphs\ntest_list_a = []\nfor i, (_, row) in enumerate(tqdm(test_df.iterrows(), total=len(test_df))):\n    g = build_graph_version_a(row, verbose=(i < 3))\n    if g: test_list_a.append(g)\n\nprint(f\" {len(graph_list_a)} training graphs |  {len(test_list_a)} test graphs\")\n\n# Loaders\ntrain_loader_a = DataLoader(graph_list_a, batch_size=16, shuffle=True)\ntest_loader_a = DataLoader(test_list_a, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:31:09.867234Z","iopub.execute_input":"2025-05-06T17:31:09.867923Z","iopub.status.idle":"2025-05-06T17:47:57.508801Z","shell.execute_reply.started":"2025-05-06T17:31:09.867897Z","shell.execute_reply":"2025-05-06T17:47:57.508010Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5208/5208 [13:28<00:00,  6.44it/s]\n100%|██████████| 1302/1302 [03:19<00:00,  6.53it/s]","output_type":"stream"},{"name":"stdout","text":" 5208 training graphs |  1302 test graphs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\nimport torch.nn as nn\n\nclass PersonalityBERTGNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(768, 256)\n        self.bn1 = BatchNorm(256)\n        self.conv2 = GCNConv(256, 128)\n        self.bn2 = BatchNorm(128)\n        self.conv3 = GCNConv(128, 64)\n        self.bn3 = BatchNorm(64)\n        self.dropout = nn.Dropout(0.3)\n        self.mood_head = nn.Linear(64, 768)\n        self.emotion_head = nn.Linear(64, 7)\n\n    def forward(self, x, edge_index, batch):\n        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n        x = self.dropout(x)\n        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n        x = self.dropout(x)\n        x = F.relu(self.bn3(self.conv3(x, edge_index)))\n        x = global_mean_pool(x, batch)\n        return self.mood_head(x), self.emotion_head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:59:20.186487Z","iopub.execute_input":"2025-05-06T17:59:20.187129Z","iopub.status.idle":"2025-05-06T17:59:20.192763Z","shell.execute_reply.started":"2025-05-06T17:59:20.187103Z","shell.execute_reply":"2025-05-06T17:59:20.192024Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def train_model(model, train_loader, save_path=\"version_a_gnn.pt\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    loss_mood = nn.MSELoss()\n    loss_emotion = nn.CrossEntropyLoss()\n\n    print(\" Training\")\n    for epoch in range(1, 251):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            mood_pred, emotion_pred = model(batch.x, batch.edge_index, batch.batch)\n            mood_targets = torch.stack([g.mood for g in batch.to_data_list()]).to(device)\n            loss = loss_emotion(emotion_pred, batch.y) + loss_mood(mood_pred, mood_targets)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch:03d} | Total Loss: {total_loss:.4f}\")\n\n    torch.save(model.state_dict(), save_path)\n    print(f\" Model saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:59:20.193365Z","iopub.execute_input":"2025-05-06T17:59:20.193530Z","iopub.status.idle":"2025-05-06T17:59:20.217747Z","shell.execute_reply.started":"2025-05-06T17:59:20.193517Z","shell.execute_reply":"2025-05-06T17:59:20.216976Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, classification_report\n\ndef evaluate_model(model_class, model_path, test_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model_class()\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n\n    preds, labels, mood_losses = [], [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch.to(device)\n            mood_pred, emotion_pred = model(batch.x, batch.edge_index, batch.batch)\n            preds.extend(torch.argmax(emotion_pred, dim=1).cpu().tolist())\n            labels.extend(batch.y.cpu().tolist())\n            mood_targets = torch.stack([g.mood for g in batch.to_data_list()]).to(device)\n            mood_losses.append(F.mse_loss(mood_pred, mood_targets).item())\n\n    print(\"\\n Evaluation — VERSION A\")\n    print(\"Mood MSE:\", np.mean(mood_losses))\n    print(\"Accuracy:\", accuracy_score(labels, preds))\n    print(\"F1 Macro:\", f1_score(labels, preds, average='macro'))\n    print(\"F1 Micro:\", f1_score(labels, preds, average='micro'))\n    print(\"F1 Weighted:\", f1_score(labels, preds, average='weighted'))\n    print(\"\\nClassification Report:\")\n    print(classification_report(labels, preds, target_names=list(emotion2id.keys())))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:59:20.218510Z","iopub.execute_input":"2025-05-06T17:59:20.218790Z","iopub.status.idle":"2025-05-06T17:59:20.235795Z","shell.execute_reply.started":"2025-05-06T17:59:20.218773Z","shell.execute_reply":"2025-05-06T17:59:20.235161Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"model = PersonalityBERTGNN()\ntrain_model(model, train_loader_a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:47:57.555717Z","iopub.execute_input":"2025-05-06T17:47:57.555961Z","iopub.status.idle":"2025-05-06T17:59:20.184932Z","shell.execute_reply.started":"2025-05-06T17:47:57.555946Z","shell.execute_reply":"2025-05-06T17:59:20.184200Z"}},"outputs":[{"name":"stdout","text":" Training\nEpoch 001 | Total Loss: 563.3496\nEpoch 002 | Total Loss: 548.8077\nEpoch 003 | Total Loss: 542.7957\nEpoch 004 | Total Loss: 538.1893\nEpoch 005 | Total Loss: 533.3124\nEpoch 006 | Total Loss: 531.7167\nEpoch 007 | Total Loss: 524.8073\nEpoch 008 | Total Loss: 519.7132\nEpoch 009 | Total Loss: 514.9044\nEpoch 010 | Total Loss: 509.4402\nEpoch 011 | Total Loss: 501.1593\nEpoch 012 | Total Loss: 494.1062\nEpoch 013 | Total Loss: 481.7312\nEpoch 014 | Total Loss: 474.6150\nEpoch 015 | Total Loss: 461.1701\nEpoch 016 | Total Loss: 450.7156\nEpoch 017 | Total Loss: 439.2836\nEpoch 018 | Total Loss: 427.1819\nEpoch 019 | Total Loss: 415.9321\nEpoch 020 | Total Loss: 401.3554\nEpoch 021 | Total Loss: 389.9171\nEpoch 022 | Total Loss: 377.3811\nEpoch 023 | Total Loss: 366.7532\nEpoch 024 | Total Loss: 356.4189\nEpoch 025 | Total Loss: 342.2206\nEpoch 026 | Total Loss: 332.3707\nEpoch 027 | Total Loss: 325.7061\nEpoch 028 | Total Loss: 309.3976\nEpoch 029 | Total Loss: 303.7727\nEpoch 030 | Total Loss: 292.3790\nEpoch 031 | Total Loss: 296.1291\nEpoch 032 | Total Loss: 280.0551\nEpoch 033 | Total Loss: 275.4797\nEpoch 034 | Total Loss: 269.1160\nEpoch 035 | Total Loss: 259.0461\nEpoch 036 | Total Loss: 251.3378\nEpoch 037 | Total Loss: 251.6280\nEpoch 038 | Total Loss: 248.4805\nEpoch 039 | Total Loss: 244.7848\nEpoch 040 | Total Loss: 231.3297\nEpoch 041 | Total Loss: 238.8740\nEpoch 042 | Total Loss: 231.1120\nEpoch 043 | Total Loss: 227.1377\nEpoch 044 | Total Loss: 222.1381\nEpoch 045 | Total Loss: 218.7425\nEpoch 046 | Total Loss: 212.0919\nEpoch 047 | Total Loss: 216.2194\nEpoch 048 | Total Loss: 209.2422\nEpoch 049 | Total Loss: 207.6122\nEpoch 050 | Total Loss: 205.8863\nEpoch 051 | Total Loss: 206.4391\nEpoch 052 | Total Loss: 199.4625\nEpoch 053 | Total Loss: 200.0159\nEpoch 054 | Total Loss: 194.8411\nEpoch 055 | Total Loss: 198.8241\nEpoch 056 | Total Loss: 194.7411\nEpoch 057 | Total Loss: 189.9635\nEpoch 058 | Total Loss: 187.1492\nEpoch 059 | Total Loss: 183.7802\nEpoch 060 | Total Loss: 189.7731\nEpoch 061 | Total Loss: 181.8880\nEpoch 062 | Total Loss: 184.2862\nEpoch 063 | Total Loss: 179.1717\nEpoch 064 | Total Loss: 184.8098\nEpoch 065 | Total Loss: 183.2459\nEpoch 066 | Total Loss: 177.9948\nEpoch 067 | Total Loss: 176.3273\nEpoch 068 | Total Loss: 168.5032\nEpoch 069 | Total Loss: 169.1670\nEpoch 070 | Total Loss: 170.3700\nEpoch 071 | Total Loss: 174.9087\nEpoch 072 | Total Loss: 174.0821\nEpoch 073 | Total Loss: 171.2700\nEpoch 074 | Total Loss: 166.8337\nEpoch 075 | Total Loss: 160.3486\nEpoch 076 | Total Loss: 163.8198\nEpoch 077 | Total Loss: 172.7589\nEpoch 078 | Total Loss: 164.5842\nEpoch 079 | Total Loss: 164.5187\nEpoch 080 | Total Loss: 166.9436\nEpoch 081 | Total Loss: 154.4982\nEpoch 082 | Total Loss: 158.8416\nEpoch 083 | Total Loss: 169.2350\nEpoch 084 | Total Loss: 158.5054\nEpoch 085 | Total Loss: 159.5209\nEpoch 086 | Total Loss: 155.8148\nEpoch 087 | Total Loss: 155.7628\nEpoch 088 | Total Loss: 157.5567\nEpoch 089 | Total Loss: 152.4715\nEpoch 090 | Total Loss: 158.8056\nEpoch 091 | Total Loss: 151.3260\nEpoch 092 | Total Loss: 153.7574\nEpoch 093 | Total Loss: 150.2348\nEpoch 094 | Total Loss: 150.7529\nEpoch 095 | Total Loss: 148.9547\nEpoch 096 | Total Loss: 149.3982\nEpoch 097 | Total Loss: 153.3184\nEpoch 098 | Total Loss: 152.8496\nEpoch 099 | Total Loss: 149.9167\nEpoch 100 | Total Loss: 146.1524\nEpoch 101 | Total Loss: 150.8780\nEpoch 102 | Total Loss: 149.8577\nEpoch 103 | Total Loss: 150.8939\nEpoch 104 | Total Loss: 140.5716\nEpoch 105 | Total Loss: 142.0575\nEpoch 106 | Total Loss: 152.6566\nEpoch 107 | Total Loss: 151.9742\nEpoch 108 | Total Loss: 140.7267\nEpoch 109 | Total Loss: 143.5171\nEpoch 110 | Total Loss: 138.1455\nEpoch 111 | Total Loss: 147.8118\nEpoch 112 | Total Loss: 145.0271\nEpoch 113 | Total Loss: 141.5360\nEpoch 114 | Total Loss: 138.2959\nEpoch 115 | Total Loss: 139.1002\nEpoch 116 | Total Loss: 141.6446\nEpoch 117 | Total Loss: 143.3812\nEpoch 118 | Total Loss: 144.2254\nEpoch 119 | Total Loss: 140.3355\nEpoch 120 | Total Loss: 133.7985\nEpoch 121 | Total Loss: 137.3482\nEpoch 122 | Total Loss: 144.7024\nEpoch 123 | Total Loss: 140.5134\nEpoch 124 | Total Loss: 138.4803\nEpoch 125 | Total Loss: 137.0797\nEpoch 126 | Total Loss: 136.2112\nEpoch 127 | Total Loss: 137.1943\nEpoch 128 | Total Loss: 128.5695\nEpoch 129 | Total Loss: 135.8697\nEpoch 130 | Total Loss: 134.8892\nEpoch 131 | Total Loss: 136.7915\nEpoch 132 | Total Loss: 130.7863\nEpoch 133 | Total Loss: 131.0601\nEpoch 134 | Total Loss: 129.4575\nEpoch 135 | Total Loss: 131.0808\nEpoch 136 | Total Loss: 133.7278\nEpoch 137 | Total Loss: 128.5158\nEpoch 138 | Total Loss: 133.0586\nEpoch 139 | Total Loss: 131.8766\nEpoch 140 | Total Loss: 131.9411\nEpoch 141 | Total Loss: 127.3119\nEpoch 142 | Total Loss: 127.6581\nEpoch 143 | Total Loss: 132.1804\nEpoch 144 | Total Loss: 133.0685\nEpoch 145 | Total Loss: 134.1231\nEpoch 146 | Total Loss: 127.9497\nEpoch 147 | Total Loss: 124.9175\nEpoch 148 | Total Loss: 126.1712\nEpoch 149 | Total Loss: 128.1258\nEpoch 150 | Total Loss: 125.3090\nEpoch 151 | Total Loss: 127.6992\nEpoch 152 | Total Loss: 128.1104\nEpoch 153 | Total Loss: 127.7609\nEpoch 154 | Total Loss: 125.1060\nEpoch 155 | Total Loss: 127.1924\nEpoch 156 | Total Loss: 123.8035\nEpoch 157 | Total Loss: 129.8977\nEpoch 158 | Total Loss: 117.3538\nEpoch 159 | Total Loss: 122.0362\nEpoch 160 | Total Loss: 126.0899\nEpoch 161 | Total Loss: 121.0183\nEpoch 162 | Total Loss: 130.4271\nEpoch 163 | Total Loss: 121.1337\nEpoch 164 | Total Loss: 119.8502\nEpoch 165 | Total Loss: 126.8275\nEpoch 166 | Total Loss: 121.7970\nEpoch 167 | Total Loss: 119.0755\nEpoch 168 | Total Loss: 118.8500\nEpoch 169 | Total Loss: 120.4064\nEpoch 170 | Total Loss: 130.4278\nEpoch 171 | Total Loss: 118.9980\nEpoch 172 | Total Loss: 115.5816\nEpoch 173 | Total Loss: 123.6643\nEpoch 174 | Total Loss: 117.7268\nEpoch 175 | Total Loss: 121.6179\nEpoch 176 | Total Loss: 121.9036\nEpoch 177 | Total Loss: 120.3773\nEpoch 178 | Total Loss: 117.7697\nEpoch 179 | Total Loss: 116.2010\nEpoch 180 | Total Loss: 119.6675\nEpoch 181 | Total Loss: 120.8293\nEpoch 182 | Total Loss: 120.7784\nEpoch 183 | Total Loss: 117.2545\nEpoch 184 | Total Loss: 115.2577\nEpoch 185 | Total Loss: 117.6085\nEpoch 186 | Total Loss: 119.0249\nEpoch 187 | Total Loss: 118.4552\nEpoch 188 | Total Loss: 117.6032\nEpoch 189 | Total Loss: 115.2172\nEpoch 190 | Total Loss: 113.1235\nEpoch 191 | Total Loss: 116.4438\nEpoch 192 | Total Loss: 120.1242\nEpoch 193 | Total Loss: 118.8326\nEpoch 194 | Total Loss: 114.2492\nEpoch 195 | Total Loss: 120.9332\nEpoch 196 | Total Loss: 110.5302\nEpoch 197 | Total Loss: 116.8856\nEpoch 198 | Total Loss: 112.1061\nEpoch 199 | Total Loss: 118.0321\nEpoch 200 | Total Loss: 112.9427\nEpoch 201 | Total Loss: 113.4862\nEpoch 202 | Total Loss: 108.5537\nEpoch 203 | Total Loss: 113.0433\nEpoch 204 | Total Loss: 112.8878\nEpoch 205 | Total Loss: 120.3333\nEpoch 206 | Total Loss: 119.1062\nEpoch 207 | Total Loss: 114.3611\nEpoch 208 | Total Loss: 111.6611\nEpoch 209 | Total Loss: 106.2862\nEpoch 210 | Total Loss: 111.3979\nEpoch 211 | Total Loss: 109.9557\nEpoch 212 | Total Loss: 115.5844\nEpoch 213 | Total Loss: 107.4925\nEpoch 214 | Total Loss: 108.6623\nEpoch 215 | Total Loss: 111.4203\nEpoch 216 | Total Loss: 111.4210\nEpoch 217 | Total Loss: 102.8195\nEpoch 218 | Total Loss: 111.1690\nEpoch 219 | Total Loss: 105.4175\nEpoch 220 | Total Loss: 107.3616\nEpoch 221 | Total Loss: 111.3009\nEpoch 222 | Total Loss: 109.6938\nEpoch 223 | Total Loss: 109.3121\nEpoch 224 | Total Loss: 106.9337\nEpoch 225 | Total Loss: 109.0463\nEpoch 226 | Total Loss: 105.1550\nEpoch 227 | Total Loss: 114.6685\nEpoch 228 | Total Loss: 105.7920\nEpoch 229 | Total Loss: 108.4878\nEpoch 230 | Total Loss: 112.6855\nEpoch 231 | Total Loss: 108.2118\nEpoch 232 | Total Loss: 105.9775\nEpoch 233 | Total Loss: 104.5323\nEpoch 234 | Total Loss: 108.3833\nEpoch 235 | Total Loss: 104.4224\nEpoch 236 | Total Loss: 96.4506\nEpoch 237 | Total Loss: 101.9135\nEpoch 238 | Total Loss: 105.8460\nEpoch 239 | Total Loss: 101.0664\nEpoch 240 | Total Loss: 100.8869\nEpoch 241 | Total Loss: 106.6183\nEpoch 242 | Total Loss: 104.2916\nEpoch 243 | Total Loss: 103.4358\nEpoch 244 | Total Loss: 103.1182\nEpoch 245 | Total Loss: 101.7158\nEpoch 246 | Total Loss: 98.5872\nEpoch 247 | Total Loss: 100.5412\nEpoch 248 | Total Loss: 98.5644\nEpoch 249 | Total Loss: 104.6729\nEpoch 250 | Total Loss: 105.4808\n Model saved to version_a_gnn.pt\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"evaluate_model(PersonalityBERTGNN, \"version_a_gnn.pt\", test_loader_a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T18:01:31.569654Z","iopub.execute_input":"2025-05-06T18:01:31.570426Z","iopub.status.idle":"2025-05-06T18:01:32.123035Z","shell.execute_reply.started":"2025-05-06T18:01:31.570400Z","shell.execute_reply":"2025-05-06T18:01:32.122390Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2709839988.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"\n Evaluation — VERSION A\nMood MSE: 0.05350643637158522\nAccuracy: 0.32181259600614437\nF1 Macro: 0.18896991381430697\nF1 Micro: 0.32181259600614437\nF1 Weighted: 0.29996897325417193\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       anger       0.26      0.24      0.25       171\n     disgust       0.08      0.03      0.05        29\n        fear       0.14      0.10      0.12        97\n         joy       0.18      0.16      0.17       225\n     neutral       0.43      0.56      0.49       554\n     sadness       0.25      0.14      0.18        99\n    surprise       0.08      0.06      0.07       127\n\n    accuracy                           0.32      1302\n   macro avg       0.20      0.19      0.19      1302\nweighted avg       0.29      0.32      0.30      1302\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}